{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radicalcoder/ML-Project/blob/master/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWq7yyS0HsIR",
        "colab_type": "code",
        "outputId": "68d3386c-7201-446e-fff0-5274b1b9cbdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage\n",
        "import skimage.data\n",
        "import skimage.transform\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import itertools\n",
        "from keras import models\n",
        "from keras.preprocessing import image\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "import keras\n",
        "import PIL\n",
        "from skimage import io\n",
        "from skimage import color\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
        "\n",
        "\n",
        "\n",
        "# To read the data directory from repository on github.\n",
        "%matplotlib inline\n",
        "!git clone -l -s https://github.com/radicalcoder/ML-Project.git data\n",
        "%cd data\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'data'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/60)\u001b[K\rremote: Counting objects:   3% (2/60)\u001b[K\rremote: Counting objects:   5% (3/60)\u001b[K\rremote: Counting objects:   6% (4/60)\u001b[K\rremote: Counting objects:   8% (5/60)\u001b[K\rremote: Counting objects:  10% (6/60)\u001b[K\rremote: Counting objects:  11% (7/60)\u001b[K\rremote: Counting objects:  13% (8/60)\u001b[K\rremote: Counting objects:  15% (9/60)\u001b[K\rremote: Counting objects:  16% (10/60)\u001b[K\rremote: Counting objects:  18% (11/60)\u001b[K\rremote: Counting objects:  20% (12/60)\u001b[K\rremote: Counting objects:  21% (13/60)\u001b[K\rremote: Counting objects:  23% (14/60)\u001b[K\rremote: Counting objects:  25% (15/60)\u001b[K\rremote: Counting objects:  26% (16/60)\u001b[K\rremote: Counting objects:  28% (17/60)\u001b[K\rremote: Counting objects:  30% (18/60)\u001b[K\rremote: Counting objects:  31% (19/60)\u001b[K\rremote: Counting objects:  33% (20/60)\u001b[K\rremote: Counting objects:  35% (21/60)\u001b[K\rremote: Counting objects:  36% (22/60)\u001b[K\rremote: Counting objects:  38% (23/60)\u001b[K\rremote: Counting objects:  40% (24/60)\u001b[K\rremote: Counting objects:  41% (25/60)\u001b[K\rremote: Counting objects:  43% (26/60)\u001b[K\rremote: Counting objects:  45% (27/60)\u001b[K\rremote: Counting objects:  46% (28/60)\u001b[K\rremote: Counting objects:  48% (29/60)\u001b[K\rremote: Counting objects:  50% (30/60)\u001b[K\rremote: Counting objects:  51% (31/60)\u001b[K\rremote: Counting objects:  53% (32/60)\u001b[K\rremote: Counting objects:  55% (33/60)\u001b[K\rremote: Counting objects:  56% (34/60)\u001b[K\rremote: Counting objects:  58% (35/60)\u001b[K\rremote: Counting objects:  60% (36/60)\u001b[K\rremote: Counting objects:  61% (37/60)\u001b[K\rremote: Counting objects:  63% (38/60)\u001b[K\rremote: Counting objects:  65% (39/60)\u001b[K\rremote: Counting objects:  66% (40/60)\u001b[K\rremote: Counting objects:  68% (41/60)\u001b[K\rremote: Counting objects:  70% (42/60)\u001b[K\rremote: Counting objects:  71% (43/60)\u001b[K\rremote: Counting objects:  73% (44/60)\u001b[K\rremote: Counting objects:  75% (45/60)\u001b[K\rremote: Counting objects:  76% (46/60)\u001b[K\rremote: Counting objects:  78% (47/60)\u001b[K\rremote: Counting objects:  80% (48/60)\u001b[K\rremote: Counting objects:  81% (49/60)\u001b[K\rremote: Counting objects:  83% (50/60)\u001b[K\rremote: Counting objects:  85% (51/60)\u001b[K\rremote: Counting objects:  86% (52/60)\u001b[K\rremote: Counting objects:  88% (53/60)\u001b[K\rremote: Counting objects:  90% (54/60)\u001b[K\rremote: Counting objects:  91% (55/60)\u001b[K\rremote: Counting objects:  93% (56/60)\u001b[K\rremote: Counting objects:  95% (57/60)\u001b[K\rremote: Counting objects:  96% (58/60)\u001b[K\rremote: Counting objects:  98% (59/60)\u001b[K\rremote: Counting objects: 100% (60/60)\u001b[K\rremote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 7409 (delta 36), reused 0 (delta 0), pack-reused 7349\u001b[K\n",
            "Receiving objects: 100% (7409/7409), 248.13 MiB | 33.24 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "Checking out files: 100% (7225/7225), done.\n",
            "/content/data/data\n",
            "BelgiumTSC_Testing   MLP_final_initialdraft.ipynb  README.md\n",
            "BelgiumTSC_Training  MLP.ipynb\t\t\t   traffic.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP04wnwpKWTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_loader(data_dir):\n",
        "    \"\"\"\n",
        "    The data set is read into two lists, as follows:\n",
        "    list labels[] -> a list of numbers, where each number represents a unique image label.\n",
        "    list images[] -> a list of arrays, where each array represents a single image.\n",
        "    \n",
        "    The data set directory contains multiple subdirectories. We read all these \n",
        "    subdirectories into a list subdirs[]. Since the subfolders are read in a random order, the list\n",
        "    is also in a random order. We then sort this array using the sort() function.\n",
        "    \"\"\"\n",
        "    \n",
        "    subdirs = []\n",
        "    for i in os.listdir(data_dir):\n",
        "      if os.path.isdir(data_dir + i):   # There may be .txt files in the data_dir hence we need to check\n",
        "        subdirs.append(i)\n",
        "\n",
        "    subdirs.sort()\n",
        "    \n",
        "    \"\"\"\n",
        "    Data is now collected into two lists - labels & images - while looping through the \n",
        "    list subdirs[].    \n",
        "    \"\"\"\n",
        "\n",
        "    labels = []\n",
        "    images = []\n",
        "    for i in subdirs:\n",
        "        label_dir = data_dir + i\n",
        "        #print(label_dir)\n",
        "        f_names=[]\n",
        "        for f in os.listdir(label_dir):\n",
        "          if f.endswith(\".ppm\"):   # only files that end in .ppm are image files\n",
        "            f_names.append(label_dir + '/' + f)\n",
        "        # print(f_names)\n",
        "        \"\"\"\n",
        "        Now for each label, append all its images into the list images[] and \n",
        "        append its label number in the list labels[]. \n",
        "        \"\"\"\n",
        "        \n",
        "        for f in f_names:\n",
        "            images.append(skimage.data.imread(f))   #use parameter as_grey??\n",
        "            labels.append(int(i))\n",
        "    return labels, images\n",
        "\n",
        "\n",
        "# Loading the training and testing dataset using the function data_loader().\n",
        "training_data_dir = \"BelgiumTSC_Training/Training/\"\n",
        "testing_data_dir = \"BelgiumTSC_Testing/Testing/\"\n",
        "\n",
        "training_labels, training_images = data_loader(training_data_dir)\n",
        "testing_labels, testing_images = data_loader(testing_data_dir)\n",
        "#print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWYP-F1pLS3w",
        "colab_type": "code",
        "outputId": "74dadb71-948d-41e7-c9f0-2adb68482f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"No of unique Labels in Training Data: \", len(set(training_labels)), \"\\nTotal number of Images in Training Data: \", len(training_images)) \n",
        "print(\"No of unique Labels in Testing Data: \", len(set(testing_labels)), \"\\nTotal number of Images in Testing Data: \", len(testing_images)) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of unique Labels in Training Data:  62 \n",
            "Total number of Images in Training Data:  4575\n",
            "No of unique Labels in Testing Data:  53 \n",
            "Total number of Images in Testing Data:  2520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KphYIFdfYZgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_images_and_labels(images, labels):\n",
        "    #Display the first image of each label.\n",
        "    unique_labels = set(labels)\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    i = 1\n",
        "    for label in unique_labels:\n",
        "        # Pick the first image for each label.\n",
        "        image = images[labels.index(label)]\n",
        "        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n",
        "        i += 1\n",
        "        _ = plt.imshow(image,cmap=\"gray\")\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "895tn4QSPQpK",
        "colab_type": "code",
        "outputId": "c6eb3961-9ba0-4f99-e5c3-f6eaeb5ae06a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "#Training Data\n",
        "reshape_images_training=[]\n",
        "grayscale_images_training=[]\n",
        "\n",
        "for i in training_images:\n",
        "  reshape_images_training.append(skimage.transform.resize(i,(32,32), mode ='constant'))\n",
        "\n",
        "reshape_images_training= np.asarray(reshape_images_training)\n",
        "#reshape_images_training = reshape_images_training.astype('float32')\n",
        "print(reshape_images_training.shape)\n",
        "\n",
        "for i in reshape_images_training:\n",
        "  grayscale_images_training.append(color.rgb2gray(i))\n",
        "  \n",
        "grayscale_images_training= np.asarray(grayscale_images_training)\n",
        "#grayscale_images_training = reshape_images_training.astype('float32')\n",
        "\n",
        "print(\"gray\")\n",
        "print(grayscale_images_training[0].shape)\n",
        "\n",
        "image_expanded_training = grayscale_images_training[:, :, :, np.newaxis]\n",
        "grayscale_images_training=image_expanded_training\n",
        "\n",
        "print(grayscale_images_training[0].shape)\n",
        "\n",
        "  \n",
        "\n",
        "#display_images_and_labels(training_images,training_labels)\n",
        "#display_images_and_labels(grayscale_images_training, training_labels)\n",
        "\n",
        "print(\"Grayscale Training images\")\n",
        "print(len(grayscale_images_training))\n",
        "for i in grayscale_images_training[:10]:\n",
        "  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\n",
        "\n",
        "print(\"Reshaped Training Images \")\n",
        "\n",
        "for i in reshape_images_training[:10]:\n",
        "  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4575, 32, 32, 3)\n",
            "gray\n",
            "(32, 32)\n",
            "(32, 32, 1)\n",
            "Grayscale Training images\n",
            "4575\n",
            "shape: (32, 32, 1), min: 0.011294590226715995, max: 0.8266693114276962\n",
            "shape: (32, 32, 1), min: 0.0721100235523897, max: 0.9055836569393383\n",
            "shape: (32, 32, 1), min: 0.07370136335784315, max: 0.8950602941176472\n",
            "shape: (32, 32, 1), min: 0.09544324295343169, max: 0.996078431372549\n",
            "shape: (32, 32, 1), min: 0.06889145009957118, max: 0.9915034355851715\n",
            "shape: (32, 32, 1), min: 0.052944715073529366, max: 0.9952450980392157\n",
            "shape: (32, 32, 1), min: 0.04544354166666667, max: 0.6664398580154718\n",
            "shape: (32, 32, 1), min: 0.12471631127450947, max: 0.9959773720894608\n",
            "shape: (32, 32, 1), min: 0.054898770105698505, max: 0.9739451960784319\n",
            "shape: (32, 32, 1), min: 0.024818349705116323, max: 0.6964905469707414\n",
            "Reshaped Training Images \n",
            "shape: (32, 32, 3), min: 0.005943627450980504, max: 0.8743221507352942\n",
            "shape: (32, 32, 3), min: 0.06296913296568632, max: 0.9191425398284312\n",
            "shape: (32, 32, 3), min: 0.060355392156862836, max: 0.9028492647058821\n",
            "shape: (32, 32, 3), min: 0.0899050245098042, max: 0.996078431372549\n",
            "shape: (32, 32, 3), min: 0.053921568627450976, max: 0.9921568627450981\n",
            "shape: (32, 32, 3), min: 0.03395373774509811, max: 0.996078431372549\n",
            "shape: (32, 32, 3), min: 0.04338235294117647, max: 0.7008779488357842\n",
            "shape: (32, 32, 3), min: 0.08508731617647042, max: 0.996078431372549\n",
            "shape: (32, 32, 3), min: 0.05366210937499997, max: 0.9861969592524511\n",
            "shape: (32, 32, 3), min: 0.023529411764705882, max: 0.7336397058823529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0wFoRRO1Ru6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "2f7bb3a1-641d-4875-f969-1f6d40c18353"
      },
      "source": [
        "#Testing Data\n",
        "reshape_images_testing=[]\n",
        "grayscale_images_testing=[]\n",
        "\n",
        "for i in testing_images:\n",
        "  reshape_images_testing.append(skimage.transform.resize(i,(32,32), mode ='constant'))\n",
        "\n",
        "reshape_images_testing= np.asarray(reshape_images_testing)\n",
        "#reshape_images_testing = reshape_images_testing.astype('float32')\n",
        "print(reshape_images_testing.shape)\n",
        "\n",
        "for i in reshape_images_testing:\n",
        "  grayscale_images_testing.append(color.rgb2gray(i))\n",
        "  \n",
        "grayscale_images_testing= np.asarray(grayscale_images_testing)\n",
        "#grayscale_images_testing = reshape_images_testing.astype('float32')\n",
        "\n",
        "print(\"gray\")\n",
        "print(grayscale_images_testing[0].shape)\n",
        "\n",
        "image_expanded_testing = grayscale_images_testing[:, :, :, np.newaxis]\n",
        "grayscale_images_testing=image_expanded_testing\n",
        "\n",
        "print(grayscale_images_testing[0].shape)\n",
        "\n",
        "  \n",
        "\n",
        "#display_images_and_labels(testing_images,testing_labels)\n",
        "#display_images_and_labels(grayscale_images_testing, testing_labels)\n",
        "\n",
        "print(\"Grayscale testing images\")\n",
        "print(len(grayscale_images_testing))\n",
        "for i in grayscale_images_testing[:10]:\n",
        "  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\n",
        "\n",
        "print(\"Reshaped testing Images \")\n",
        "\n",
        "for i in reshape_images_testing[:10]:\n",
        "  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2520, 32, 32, 3)\n",
            "gray\n",
            "(32, 32)\n",
            "(32, 32, 1)\n",
            "Grayscale testing images\n",
            "2520\n",
            "shape: (32, 32, 1), min: 0.07546836243872548, max: 0.924712867647059\n",
            "shape: (32, 32, 1), min: 0.11455873314951014, max: 0.996078431372549\n",
            "shape: (32, 32, 1), min: 0.0706251221660539, max: 0.9149987683823528\n",
            "shape: (32, 32, 1), min: 0.09921841500076546, max: 0.9968231752642462\n",
            "shape: (32, 32, 1), min: 0.11204302523743893, max: 0.9942511990655637\n",
            "shape: (32, 32, 1), min: 0.07224462775735288, max: 0.8842977512254903\n",
            "shape: (32, 32, 1), min: 0.03030558823529412, max: 0.996078431372549\n",
            "shape: (32, 32, 1), min: 0.015693348268995277, max: 0.996078431372549\n",
            "shape: (32, 32, 1), min: 0.03448774892769608, max: 0.9921568627450981\n",
            "shape: (32, 32, 1), min: 0.06583969247855377, max: 0.996078431372549\n",
            "Reshaped testing Images \n",
            "shape: (32, 32, 3), min: 0.05526960784313725, max: 0.9752987132352939\n",
            "shape: (32, 32, 3), min: 0.05646829044117647, max: 0.996078431372549\n",
            "shape: (32, 32, 3), min: 0.058452052696078415, max: 0.9610906862745097\n",
            "shape: (32, 32, 3), min: 0.04930683210784309, max: 0.9995710784313726\n",
            "shape: (32, 32, 3), min: 0.07037568933823561, max: 0.996078431372549\n",
            "shape: (32, 32, 3), min: 0.04664905024509794, max: 0.9242378982843135\n",
            "shape: (32, 32, 3), min: 0.017302389705882354, max: 0.996078431372549\n",
            "shape: (32, 32, 3), min: 0.010022212009804212, max: 0.996078431372549\n",
            "shape: (32, 32, 3), min: 0.028347120098039184, max: 0.9921568627450981\n",
            "shape: (32, 32, 3), min: 0.03743872549019619, max: 0.996078431372549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdOUPLWL-S0U",
        "colab_type": "code",
        "outputId": "8ca1f0d2-2141-4ba3-ce0c-d881226b1b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "print(training_labels)\n",
        "from keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "labels_tr = to_categorical(training_labels)\n",
        "labels_test = to_categorical(testing_labels)\n",
        "print(labels_tr.shape)\n",
        "print(\"\\n\")\n",
        "print(labels_tr)\n",
        "print(training_labels[500])\n",
        "print(labels_tr[500])\n",
        "\n",
        "print(testing_labels)\n",
        "print(labels_test.shape)\n",
        "print(\"\\n\")\n",
        "print(labels_test)\n",
        "print(testing_labels[500])\n",
        "print(labels_test[500])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
            "(4575, 62)\n",
            "\n",
            "\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "13\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 49, 49, 49, 51, 51, 51, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61]\n",
            "(2520, 62)\n",
            "\n",
            "\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "18\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhHs9qrNAV5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "model_gen = Sequential()\n",
        "model_gen.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=grayscale_images_training[0].shape))\n",
        "#model_gen.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model_gen.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_gen.add(Dropout(0.25))\n",
        " \n",
        "model_gen.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "#model_gen.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model_gen.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_gen.add(Dropout(0.25))\n",
        " \n",
        "model_gen.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "#model_gen.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model_gen.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_gen.add(Dropout(0.25))\n",
        " \n",
        "model_gen.add(Flatten())\n",
        "model_gen.add(Dense(512, activation='relu')) # we can drop \n",
        "model_gen.add(Dropout(0.5))                  # this layers\n",
        "model_gen.add(Dense(512, activation='relu'))\n",
        "model_gen.add(Dropout(0.5))\n",
        "model_gen.add(Dense(62, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JE-NktqGCzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_gen.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM_5WeumGPmJ",
        "colab_type": "code",
        "outputId": "279c0c25-352c-4b02-975b-2e9f46ad4cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# we can use image augmentation\n",
        "# basically it needs to redifine for normal actual scores like 0.9 of accuracy and more\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=12,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "datagen.fit(grayscale_images_training)\n",
        "\n",
        "\n",
        "%time\n",
        "epochs=50\n",
        "batch_size = 20\n",
        "history_generator = model_gen.fit_generator(datagen.flow(grayscale_images_training, labels_tr, batch_size=batch_size),\n",
        "                    epochs=epochs, steps_per_epoch=500 , verbose=1,\n",
        "                    validation_data=(grayscale_images_testing, labels_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 4.77 µs\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.9534 - acc: 0.5431 - val_loss: 2.4926 - val_acc: 0.3762\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.9899 - acc: 0.5446 - val_loss: 2.1590 - val_acc: 0.4520\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.0218 - acc: 0.5274 - val_loss: 1.9897 - val_acc: 0.5075\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.0418 - acc: 0.5318 - val_loss: 2.0806 - val_acc: 0.4889\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.0369 - acc: 0.5303 - val_loss: 2.5477 - val_acc: 0.3675\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.0074 - acc: 0.5306 - val_loss: 2.6475 - val_acc: 0.3437\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.0495 - acc: 0.5293 - val_loss: 2.6477 - val_acc: 0.3254\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.0392 - acc: 0.5292 - val_loss: 2.0770 - val_acc: 0.4540\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.0668 - acc: 0.5227 - val_loss: 1.9906 - val_acc: 0.5230\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.0945 - acc: 0.5179 - val_loss: 2.4216 - val_acc: 0.4056\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.1281 - acc: 0.5077 - val_loss: 2.7005 - val_acc: 0.3175\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.1158 - acc: 0.5115 - val_loss: 1.8716 - val_acc: 0.5468\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.1294 - acc: 0.5070 - val_loss: 2.3680 - val_acc: 0.4365\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.1200 - acc: 0.4999 - val_loss: 2.0473 - val_acc: 0.5111\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.2456 - acc: 0.4958 - val_loss: 2.4703 - val_acc: 0.3750\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.1601 - acc: 0.4905 - val_loss: 2.8827 - val_acc: 0.3123\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 2.1192 - acc: 0.5143 - val_loss: 3.0642 - val_acc: 0.2948\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.1813 - acc: 0.4936 - val_loss: 1.9072 - val_acc: 0.5290\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.1654 - acc: 0.5053 - val_loss: 2.9535 - val_acc: 0.2810\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.2150 - acc: 0.4883 - val_loss: 2.3821 - val_acc: 0.4405\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.1816 - acc: 0.4956 - val_loss: 2.3481 - val_acc: 0.4171\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.2257 - acc: 0.4762 - val_loss: 2.1369 - val_acc: 0.4968\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.1839 - acc: 0.4708 - val_loss: 2.0384 - val_acc: 0.4714\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.2318 - acc: 0.4862 - val_loss: 2.3520 - val_acc: 0.4512\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.2359 - acc: 0.4693 - val_loss: 2.5770 - val_acc: 0.3976\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.2075 - acc: 0.4733 - val_loss: 2.0041 - val_acc: 0.5179\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.2111 - acc: 0.4710 - val_loss: 1.9999 - val_acc: 0.5210\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.2540 - acc: 0.4838 - val_loss: 2.4218 - val_acc: 0.4258\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.2576 - acc: 0.4818 - val_loss: 2.5642 - val_acc: 0.4091\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.2930 - acc: 0.4667 - val_loss: 1.9626 - val_acc: 0.5306\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.2535 - acc: 0.4798 - val_loss: 2.4257 - val_acc: 0.4254\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.2360 - acc: 0.4788 - val_loss: 2.3962 - val_acc: 0.3758\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.2648 - acc: 0.4747 - val_loss: 2.4084 - val_acc: 0.4171\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3665 - acc: 0.4513 - val_loss: 3.0219 - val_acc: 0.2556\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3429 - acc: 0.4587 - val_loss: 2.3756 - val_acc: 0.4179\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.3867 - acc: 0.4545 - val_loss: 2.4833 - val_acc: 0.4226\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.3028 - acc: 0.4612 - val_loss: 2.7998 - val_acc: 0.3563\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3607 - acc: 0.4561 - val_loss: 2.0691 - val_acc: 0.5032\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.2921 - acc: 0.4707 - val_loss: 2.0029 - val_acc: 0.5460\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3088 - acc: 0.4548 - val_loss: 2.6554 - val_acc: 0.3726\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3112 - acc: 0.4571 - val_loss: 2.3215 - val_acc: 0.4575\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3058 - acc: 0.4483 - val_loss: 2.0542 - val_acc: 0.5075\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.3460 - acc: 0.4524 - val_loss: 2.0810 - val_acc: 0.5036\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3668 - acc: 0.4503 - val_loss: 2.9819 - val_acc: 0.2968\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3989 - acc: 0.4481 - val_loss: 2.5642 - val_acc: 0.4167\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3098 - acc: 0.4612 - val_loss: 1.8106 - val_acc: 0.5857\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.3439 - acc: 0.4537 - val_loss: 3.2480 - val_acc: 0.2607\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.3853 - acc: 0.4508 - val_loss: 2.5649 - val_acc: 0.4206\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.4234 - acc: 0.4417 - val_loss: 4.8091 - val_acc: 0.1647\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 2.5010 - acc: 0.4191 - val_loss: 2.0286 - val_acc: 0.4937\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}