{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radicalcoder/ML-Project/blob/master/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWq7yyS0HsIR",
        "colab_type": "code",
        "outputId": "1b23d6cb-3fc1-4b92-8442-d9fd41b8c2e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage\n",
        "import skimage.data\n",
        "import skimage.transform\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import itertools\n",
        "from keras import models\n",
        "from keras.preprocessing import image\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "import keras\n",
        "import PIL\n",
        "from skimage import io\n",
        "from skimage import color\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
        "\n",
        "\n",
        "\n",
        "# To read the data directory from repository on github.\n",
        "%matplotlib inline\n",
        "!git clone -l -s https://github.com/radicalcoder/ML-Project.git data\n",
        "%cd data\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'data'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 7424 (delta 45), reused 0 (delta 0), pack-reused 7349\u001b[K\n",
            "Receiving objects: 100% (7424/7424), 248.17 MiB | 13.88 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n",
            "Checking out files: 100% (7225/7225), done.\n",
            "/content/data\n",
            "BelgiumTSC_Testing   MLP_final_initialdraft.ipynb  README.md\n",
            "BelgiumTSC_Training  MLP.ipynb\t\t\t   traffic.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP04wnwpKWTg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2d4ad962-9536-4984-d82e-6d43466e6d0d"
      },
      "source": [
        "def data_loader(data_dir):\n",
        "    \"\"\"\n",
        "    The data set is read into two lists, as follows:\n",
        "    list labels[] -> a list of numbers, where each number represents a unique image label.\n",
        "    list images[] -> a list of arrays, where each array represents a single image.\n",
        "    \n",
        "    The data set directory contains multiple subdirectories. We read all these \n",
        "    subdirectories into a list subdirs[]. Since the subfolders are read in a random order, the list\n",
        "    is also in a random order. We then sort this array using the sort() function.\n",
        "    \"\"\"\n",
        "    \n",
        "    subdirs = []\n",
        "    for i in os.listdir(data_dir):\n",
        "      if os.path.isdir(data_dir + i):   # There may be .txt files in the data_dir hence we need to check\n",
        "        subdirs.append(i)\n",
        "\n",
        "    #subdirs.sort()\n",
        "    \n",
        "    \"\"\"\n",
        "    Data is now collected into two lists - labels & images - while looping through the \n",
        "    list subdirs[].    \n",
        "    \"\"\"\n",
        "\n",
        "    labels = []\n",
        "    images = []\n",
        "    for i in subdirs:\n",
        "        label_dir = data_dir + i\n",
        "        #print(label_dir)\n",
        "        f_names=[]\n",
        "        for f in os.listdir(label_dir):\n",
        "          if f.endswith(\".ppm\"):   # only files that end in .ppm are image files\n",
        "            f_names.append(label_dir + '/' + f)\n",
        "        # print(f_names)\n",
        "        \"\"\"\n",
        "        Now for each label, append all its images into the list images[] and \n",
        "        append its label number in the list labels[]. \n",
        "        \"\"\"\n",
        "        \n",
        "        for f in f_names:\n",
        "            images.append(skimage.data.imread(f))   #use parameter as_grey??\n",
        "            labels.append(int(i))\n",
        "    return labels, images\n",
        "\n",
        "\n",
        "# Loading the training and testing dataset using the function data_loader().\n",
        "training_data_dir = \"BelgiumTSC_Training/Training/\"\n",
        "testing_data_dir = \"BelgiumTSC_Testing/Testing/\"\n",
        "\n",
        "training_labels, training_images = data_loader(training_data_dir)\n",
        "testing_labels, testing_images = data_loader(testing_data_dir)\n",
        "print(training_labels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 60, 60, 60, 60, 60, 60, 60, 60, 60, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 16, 16, 16, 16, 16, 16, 16, 16, 16, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 15, 15, 15, 15, 15, 15, 15, 15, 15, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 26, 26, 26, 26, 26, 26, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWYP-F1pLS3w",
        "colab_type": "code",
        "outputId": "99d8d2a6-a485-499b-b84a-30163dd859be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"No of unique Labels in Training Data: \", len(set(training_labels)), \"\\nTotal number of Images in Training Data: \", len(training_images)) \n",
        "print(\"No of unique Labels in Testing Data: \", len(set(testing_labels)), \"\\nTotal number of Images in Testing Data: \", len(testing_images)) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of unique Labels in Training Data:  62 \n",
            "Total number of Images in Training Data:  4575\n",
            "No of unique Labels in Testing Data:  53 \n",
            "Total number of Images in Testing Data:  2520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KphYIFdfYZgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_images_and_labels(images, labels):\n",
        "    #Display the first image of each label.\n",
        "    unique_labels = set(labels)\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    i = 1\n",
        "    for label in unique_labels:\n",
        "        # Pick the first image for each label.\n",
        "        image = images[labels.index(label)]\n",
        "        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n",
        "        i += 1\n",
        "        _ = plt.imshow(image,cmap=\"gray\")\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "895tn4QSPQpK",
        "colab_type": "code",
        "outputId": "2eb15d44-a953-40d3-9f53-bacb781dd197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#Training Data\n",
        "reshape_images_training=[]\n",
        "grayscale_images_training=[]\n",
        "\n",
        "for i in training_images:\n",
        "  reshape_images_training.append(skimage.transform.resize(i,(32,32), mode ='constant'))\n",
        "\n",
        "reshape_images_training= np.array(reshape_images_training)\n",
        "#reshape_images_training = reshape_images_training.astype('float32')\n",
        "print(reshape_images_training.shape)\n",
        "\n",
        "for i in reshape_images_training:\n",
        "  grayscale_images_training.append(color.rgb2gray(i))\n",
        "  \n",
        "grayscale_images_training= np.array(grayscale_images_training)  #Keras expects our images and labels to be in numpy arrays\n",
        "#grayscale_images_training = reshape_images_training.astype('float32')\n",
        "\n",
        "print(\"gray\")\n",
        "print(grayscale_images_training[0].shape)\n",
        "\n",
        "image_expanded_training = grayscale_images_training[:, :, :, np.newaxis]  #converting images from 32x32 to 32x32x1\n",
        "grayscale_images_training=image_expanded_training\n",
        "\n",
        "print(grayscale_images_training[0].shape)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#display_images_and_labels(training_images,training_labels)\n",
        "#display_images_and_labels(grayscale_images_training, training_labels)\n",
        "\n",
        "print(\"Grayscale Training images\")\n",
        "print(len(grayscale_images_training))\n",
        "for i in grayscale_images_training[:10]:\n",
        "  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\n",
        "\n",
        "print(\"Reshaped Training Images \")\n",
        "\n",
        "for i in reshape_images_training[:10]:\n",
        "  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\n",
        "  \n",
        "\"\"\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4575, 32, 32, 3)\n",
            "gray\n",
            "(32, 32)\n",
            "(32, 32, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n#display_images_and_labels(training_images,training_labels)\\n#display_images_and_labels(grayscale_images_training, training_labels)\\n\\nprint(\"Grayscale Training images\")\\nprint(len(grayscale_images_training))\\nfor i in grayscale_images_training[:10]:\\n  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\\n\\nprint(\"Reshaped Training Images \")\\n\\nfor i in reshape_images_training[:10]:\\n  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\\n  \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0wFoRRO1Ru6",
        "colab_type": "code",
        "outputId": "d0910724-378e-404a-cc3d-b6470650f250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#Testing Data\n",
        "reshape_images_testing=[]\n",
        "grayscale_images_testing=[]\n",
        "\n",
        "for i in testing_images:\n",
        "  reshape_images_testing.append(skimage.transform.resize(i,(32,32), mode ='constant'))\n",
        "\n",
        "reshape_images_testing= np.array(reshape_images_testing)\n",
        "#reshape_images_testing = reshape_images_testing.astype('float32')\n",
        "print(reshape_images_testing.shape)\n",
        "\n",
        "for i in reshape_images_testing:\n",
        "  grayscale_images_testing.append(color.rgb2gray(i))\n",
        "  \n",
        "grayscale_images_testing= np.array(grayscale_images_testing)\n",
        "#grayscale_images_testing = reshape_images_testing.astype('float32')\n",
        "\n",
        "print(\"gray\")\n",
        "print(grayscale_images_testing[0].shape)\n",
        "\n",
        "image_expanded_testing = grayscale_images_testing[:, :, :, np.newaxis]  #converting imags from 32x32 to 32x32x1\n",
        "grayscale_images_testing=image_expanded_testing\n",
        "\n",
        "print(grayscale_images_testing[0].shape)\n",
        "\n",
        "  \n",
        "\n",
        "#display_images_and_labels(testing_images,testing_labels)\n",
        "#display_images_and_labels(grayscale_images_testing, testing_labels)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"Grayscale testing images\")\n",
        "print(len(grayscale_images_testing))\n",
        "for i in grayscale_images_testing[:10]:\n",
        "  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\n",
        "\n",
        "print(\"Reshaped testing Images \")\n",
        "\n",
        "for i in reshape_images_testing[:10]:\n",
        "  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2520, 32, 32, 3)\n",
            "gray\n",
            "(32, 32)\n",
            "(32, 32, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nprint(\"Grayscale testing images\")\\nprint(len(grayscale_images_testing))\\nfor i in grayscale_images_testing[:10]:\\n  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\\n\\nprint(\"Reshaped testing Images \")\\n\\nfor i in reshape_images_testing[:10]:\\n  print(\"shape: {0}, min: {1}, max: {2}\".format(i.shape, i.min(), i.max()))\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdOUPLWL-S0U",
        "colab_type": "code",
        "outputId": "d08134ba-2e29-4641-b4b4-259283f86457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#print(training_labels)\n",
        "from keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "labels_train = to_categorical(training_labels)\n",
        "labels_test = to_categorical(testing_labels)\n",
        "print(labels_train.shape)\n",
        "print(\"\\n\")\n",
        "print(type(labels_train))\n",
        "print(training_labels[500])\n",
        "print(labels_train[500])\n",
        "\n",
        "#print(testing_labels)\n",
        "print(labels_test.shape)\n",
        "print(\"\\n\")\n",
        "print(type(labels_test))\n",
        "print(testing_labels[500])\n",
        "print(labels_test[500])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4575, 62)\n",
            "\n",
            "\n",
            "<class 'numpy.ndarray'>\n",
            "61\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "(2520, 62)\n",
            "\n",
            "\n",
            "<class 'numpy.ndarray'>\n",
            "18\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhHs9qrNAV5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "model_gen = Sequential()\n",
        "model_gen.add(Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=grayscale_images_training[0].shape))  #When using this layer as the first layer in a model, we provide the keyword argument \"input_shape\"\n",
        "#model_gen.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model_gen.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_gen.add(Dropout(0.25))\n",
        " \n",
        "model_gen.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "#model_gen.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model_gen.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_gen.add(Dropout(0.25))\n",
        " \n",
        "model_gen.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "#model_gen.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model_gen.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_gen.add(Dropout(0.25))\n",
        " \n",
        "model_gen.add(Flatten())\n",
        "model_gen.add(Dense(512, activation='relu')) # we can drop \n",
        "model_gen.add(Dropout(0.5))                  # this layers\n",
        "model_gen.add(Dense(512, activation='relu'))\n",
        "model_gen.add(Dropout(0.5))\n",
        "model_gen.add(Dense(62, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JE-NktqGCzi",
        "colab_type": "code",
        "outputId": "be1a11c1-1bea-4f07-d1e6-7f53d3bc6bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "model_gen.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7_3lHXzrVmK",
        "colab_type": "code",
        "outputId": "2db0dbfb-4d6a-488a-8feb-23af573a35de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw0TTj6ovmlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating checkpoint to store the best model based on validation accuracy\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath=\"/content/drive/My Drive/ML-Proj/Best_Weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM_5WeumGPmJ",
        "colab_type": "code",
        "outputId": "37b87fb8-4590-433d-a213-cdc82a53dc62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# we can use image augmentation\n",
        "# basically it needs to redifine for normal actual scores like 0.9 of accuracy and more\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=12,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "datagen.fit(grayscale_images_training)\n",
        "\n",
        "\n",
        "%time\n",
        "epochs=50\n",
        "batch_size = 20\n",
        "history_generator = model_gen.fit_generator(datagen.flow(grayscale_images_training, labels_train, batch_size=batch_size),\n",
        "                    epochs=epochs, steps_per_epoch=500 , callbacks=callbacks_list, verbose=1,\n",
        "                    validation_data=(grayscale_images_testing, labels_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
            "Wall time: 5.01 µs\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 3.1813 - acc: 0.2078 - val_loss: 2.3716 - val_acc: 0.3929\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.39286, saving model to /content/drive/My Drive/ML-Proj/Best_Weights.hdf5\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 2.0822 - acc: 0.4606 - val_loss: 1.5326 - val_acc: 0.5571\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.39286 to 0.55714, saving model to /content/drive/My Drive/ML-Proj/Best_Weights.hdf5\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.6410 - acc: 0.5611 - val_loss: 1.5332 - val_acc: 0.5702\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.55714 to 0.57024, saving model to /content/drive/My Drive/ML-Proj/Best_Weights.hdf5\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.4187 - acc: 0.6141 - val_loss: 1.1986 - val_acc: 0.6421\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.57024 to 0.64206, saving model to /content/drive/My Drive/ML-Proj/Best_Weights.hdf5\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.2990 - acc: 0.6482 - val_loss: 1.3533 - val_acc: 0.6052\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.64206\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.2322 - acc: 0.6580 - val_loss: 1.0761 - val_acc: 0.7159\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.64206 to 0.71587, saving model to /content/drive/My Drive/ML-Proj/Best_Weights.hdf5\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.2334 - acc: 0.6645 - val_loss: 1.3377 - val_acc: 0.6603\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.71587\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.2398 - acc: 0.6668 - val_loss: 1.1613 - val_acc: 0.7067\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.71587\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.2393 - acc: 0.6702 - val_loss: 1.2761 - val_acc: 0.6679\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.71587\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.2696 - acc: 0.6685 - val_loss: 1.8751 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.71587\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.2639 - acc: 0.6655 - val_loss: 1.0736 - val_acc: 0.7278\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.71587 to 0.72778, saving model to /content/drive/My Drive/ML-Proj/Best_Weights.hdf5\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.2776 - acc: 0.6629 - val_loss: 1.4138 - val_acc: 0.6349\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.72778\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.3475 - acc: 0.6552 - val_loss: 1.9381 - val_acc: 0.5488\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.72778\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.3694 - acc: 0.6523 - val_loss: 1.3863 - val_acc: 0.6655\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.72778\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.3838 - acc: 0.6504 - val_loss: 1.6122 - val_acc: 0.6143\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.72778\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.4073 - acc: 0.6418 - val_loss: 1.3326 - val_acc: 0.6183\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.72778\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.4533 - acc: 0.6415 - val_loss: 1.5874 - val_acc: 0.6028\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.72778\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.4589 - acc: 0.6329 - val_loss: 1.8113 - val_acc: 0.5694\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.72778\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.4578 - acc: 0.6392 - val_loss: 1.7019 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.72778\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.4512 - acc: 0.6370 - val_loss: 1.3996 - val_acc: 0.6718\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.72778\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.5156 - acc: 0.6287 - val_loss: 1.6061 - val_acc: 0.6127\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.72778\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.5280 - acc: 0.6278 - val_loss: 2.0369 - val_acc: 0.5210\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.72778\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.5882 - acc: 0.6197 - val_loss: 1.9180 - val_acc: 0.5377\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.72778\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.6350 - acc: 0.6008 - val_loss: 1.8768 - val_acc: 0.5635\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.72778\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.6222 - acc: 0.6195 - val_loss: 1.5819 - val_acc: 0.6135\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.72778\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.6538 - acc: 0.6081 - val_loss: 1.8344 - val_acc: 0.5480\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.72778\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.6596 - acc: 0.6024 - val_loss: 1.9944 - val_acc: 0.5258\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.72778\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.6882 - acc: 0.5934 - val_loss: 2.8661 - val_acc: 0.2710\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.72778\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.6922 - acc: 0.5975 - val_loss: 2.4615 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.72778\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.6889 - acc: 0.5912 - val_loss: 2.0038 - val_acc: 0.4837\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.72778\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.7041 - acc: 0.5913 - val_loss: 2.0582 - val_acc: 0.5091\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.72778\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.7261 - acc: 0.5914 - val_loss: 1.9039 - val_acc: 0.5563\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.72778\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.7660 - acc: 0.5873 - val_loss: 2.9967 - val_acc: 0.3214\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.72778\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.8039 - acc: 0.5926 - val_loss: 2.1155 - val_acc: 0.5091\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.72778\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.8052 - acc: 0.5753 - val_loss: 2.3675 - val_acc: 0.4456\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.72778\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.8399 - acc: 0.5638 - val_loss: 2.3649 - val_acc: 0.4290\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.72778\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.8390 - acc: 0.5643 - val_loss: 2.8012 - val_acc: 0.3194\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.72778\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.8397 - acc: 0.5598 - val_loss: 2.2965 - val_acc: 0.4345\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.72778\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 1.8512 - acc: 0.5633 - val_loss: 2.1016 - val_acc: 0.4897\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.72778\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.9103 - acc: 0.5602 - val_loss: 2.1083 - val_acc: 0.4952\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.72778\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.8886 - acc: 0.5627 - val_loss: 1.6106 - val_acc: 0.5937\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.72778\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.9328 - acc: 0.5545 - val_loss: 1.8135 - val_acc: 0.5571\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.72778\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.9511 - acc: 0.5496 - val_loss: 2.8732 - val_acc: 0.2782\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.72778\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.9314 - acc: 0.5485 - val_loss: 2.1928 - val_acc: 0.4687\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.72778\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.9566 - acc: 0.5426 - val_loss: 2.6428 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.72778\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.9970 - acc: 0.5262 - val_loss: 1.9166 - val_acc: 0.5337\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.72778\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.9424 - acc: 0.5360 - val_loss: 2.0844 - val_acc: 0.5004\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.72778\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.9273 - acc: 0.5433 - val_loss: 2.6470 - val_acc: 0.3734\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.72778\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 2.0038 - acc: 0.5277 - val_loss: 2.6123 - val_acc: 0.3508\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.72778\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 1.9611 - acc: 0.5426 - val_loss: 3.4444 - val_acc: 0.3504\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.72778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O13y7Ks0WQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading Best Weights\n",
        "model_gen.load_weights(\"/content/drive/My Drive/ML-Proj/Best_Weights.hdf5\")\n",
        "#Saving Best Model\n",
        "model_gen.save(\"/content/drive/My Drive/ML-Proj/Best_Model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg1xE6pF2AZv",
        "colab_type": "code",
        "outputId": "7e203dde-5c3c-4108-bd83-b4968b85454d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#Predicting Traffic Signs\n",
        "y_te_pred = model_gen.predict_classes(grayscale_images_testing, batch_size=batch_size, verbose =1)\n",
        "print(y_te_pred.shape)\n",
        "print(type(y_te_pred))\n",
        "pred_value=1500\n",
        "print(y_te_pred[pred_value])\n",
        "print(testing_labels[pred_value])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2520/2520 [==============================] - 0s 160us/step\n",
            "(2520,)\n",
            "<class 'numpy.ndarray'>\n",
            "34\n",
            "19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfHn-OqU4AdN",
        "colab_type": "code",
        "outputId": "9cc36094-556d-4eeb-e46b-9926a37dd75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "scores = model_gen.evaluate(grayscale_images_testing, labels_test)\n",
        "print(scores[0])    #Loss\n",
        "print(scores[1])    #Accuracy"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2520/2520 [==============================] - 0s 142us/step\n",
            "1.0736351262223685\n",
            "0.7277777775885567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQmceQo2NchQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   \n",
        "def plot_history(history):\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
        "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
        "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
        "    \n",
        "    if len(loss_list) == 0:\n",
        "        print('Loss is missing in history')\n",
        "        return \n",
        "    \n",
        "    ## As loss always exists\n",
        "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
        "    \n",
        "    ## Loss\n",
        "    plt.figure(1)\n",
        "    for l in loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    for l in val_loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    \n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    ## Accuracy\n",
        "    plt.figure(2)\n",
        "    for l in acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "    for l in val_acc_list:    \n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=True,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        title='Normalized confusion matrix'\n",
        "    else:\n",
        "        title='Confusion matrix for 62 Sample classes'\n",
        "        \n",
        "    fig = plt.figure(figsize=(20,8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "    \n",
        "## multiclass or binary report\n",
        "## If binary (sigmoid output), set binary parameter to True\n",
        "def full_multiclass_report(model,\n",
        "                           x,\n",
        "                           y_true,\n",
        "                           classes,\n",
        "                           batch_size=32,\n",
        "                           binary=False):\n",
        "    plot_history(history_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76cGHzv9XidX",
        "colab_type": "code",
        "outputId": "4d10044e-f796-4213-9ac5-aad18caa4ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(type(labels_test))\n",
        "print(labels_test.shape)\n",
        "print(type(y_te_pred))\n",
        "print(y_te_pred[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(2520, 62)\n",
            "<class 'numpy.ndarray'>\n",
            "47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob2QkA3DQewh",
        "colab_type": "code",
        "outputId": "103f81e9-f9e5-4c8a-80a7-b7aa07c360e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "source": [
        "classes = list(range(0,62))\n",
        "y=[]\n",
        "y_pred = []\n",
        "\n",
        "for i in range(labels_test.shape[0]):\n",
        "\n",
        "  for j in range(len(classes)):\n",
        "\n",
        "    if(labels_test[i][j]==1):\n",
        "      y.append(j)\n",
        "      #print(y)\n",
        "\n",
        "\n",
        "print(len(y))\n",
        "print(y[0])\n",
        "\n",
        "#Confusion Matrix for testing data\n",
        "cm_test = confusion_matrix(y, y_te_pred)\n",
        "cm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]  #To nomalize the confusion matrix\n",
        "\n",
        "d_cm_test=np.diagonal(cm_test)\n",
        "acc_score_test = accuracy_score(y, y_te_pred)\n",
        "\n",
        "import seaborn as sns\n",
        "f = plt.figure(figsize=(20,8))\n",
        "ax=f.add_subplot(1,1,1)\n",
        "sns.heatmap(cm_test, annot=False, fmt=\".2f\", linewidths=.5, square = True, cmap = 'Blues',ax=ax, vmin=0, vmax=1)\n",
        "ax.set_xlabel('Predicted label')\n",
        "ax.set_ylabel('Actual label')\n",
        "'Accuracy Score: {0}'.format(acc_score_test)\n",
        "ax.set_title('Confusion Matrix', size = 15);\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2520\n",
            "49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAH1CAYAAADf8gvBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}